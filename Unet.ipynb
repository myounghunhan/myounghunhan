{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import tqdm\n",
    "from torch.optim.adam import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb179e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  \n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = True  \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835784ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/AI/HMH/UTRC/Dataset/1/Dataset.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b00fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_information = df.iloc[0,:]  \n",
    "print(img_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad16f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_out_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        img_in_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        image_in = Image.open(img_in_path)\n",
    "        image_out = Image.open(img_out_path)\n",
    "        label = np.array(self.img_labels.iloc[idx, 2:]).astype(np.float32)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image_in = self.transform(image_in)\n",
    "            image_out = self.transform(image_out)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image_in, image_out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((256,256)), transforms.ToTensor()])        \n",
    "target_transform = transforms.Compose([ transforms.ToTensor()])\n",
    "\n",
    "dataset = CustomDataset(annotations_file =\"/Users/AI/HMH/UTRC/Dataset/1/Dataset.csv\",    \n",
    "                        img_dir = \"/Users/AI/HMH/UTRC/Dataset/1/Image/\", transform=transform)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.9)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ab324",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = train_dataset.__getitem__(4)[1]\n",
    "print(t1.dtype)\n",
    "print(t1)\n",
    "plt.imshow(t1.transpose(2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb25431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):                   \n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.enc1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 =nn.MaxPool2d(kernel_size=2, stride=2)                  # 256 -> 128     \n",
    "        \n",
    "        self.enc2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 =nn.MaxPool2d(kernel_size=2, stride=2)                  # 128 -> 64 \n",
    "        \n",
    "        self.enc3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 =nn.MaxPool2d(kernel_size=2, stride=2)                  # 64 -> 32\n",
    "        \n",
    "        self.enc4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.enc4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool4 =nn.MaxPool2d(kernel_size=2, stride=2)                  # 32 -> 16 \n",
    "        \n",
    "        self.enc5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.enc5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.pool5 =nn.MaxPool2d(kernel_size=2, stride=2)                  # 16 -> 8 \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc0 = nn.Linear(in_features=32768, out_features=64 )                 \n",
    "        self.fc1 = nn.Linear(in_features=64+5, out_features=1024 )\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=1024 )\n",
    "        self.fc3 = nn.Linear(in_features=1024, out_features=512 )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        self.upsample5 = nn.ConvTranspose2d(512, 512, 2, stride=2)\n",
    "        self.dec5_1 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.dec5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upsample4 = nn.ConvTranspose2d(512, 512, 2, stride=2)\n",
    "        self.dec4_1 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.dec4_2 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upsample3 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
    "        self.dec3_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec3_2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upsample2 = nn.ConvTranspose2d(128, 128, 2, stride=2)\n",
    "        self.dec2_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec2_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.upsample1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.dec1_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.dec1_3 = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x ,y):\n",
    "        \n",
    "        # Encoder Network\n",
    "        x = self.enc1_1(x)\n",
    "        x = self.relu(x)\n",
    "        e1 = self.enc1_2(x)\n",
    "        e1 = self.relu(e1)         # 256\n",
    "        x = self.pool1(e1)         # 256 --> 128\n",
    "        \n",
    "        x = self.enc2_1(x)\n",
    "        x = self.relu(x)\n",
    "        e2 = self.enc2_2(x)\n",
    "        e2 = self.relu(e2)         # 128 --> 128\n",
    "        x = self.pool2(e2)         # 128 --> 64\n",
    "        \n",
    "        x = self.enc3_1(x)\n",
    "        x = self.relu(x)\n",
    "        e3 = self.enc3_2(x)\n",
    "        e3 = self.relu(e3)         # 64 --> 64\n",
    "        x = self.pool3(e3)         # 64 --> 32\n",
    "        \n",
    "        x = self.enc4_1(x)\n",
    "        x = self.relu(x)\n",
    "        e4 = self.enc4_2(x)\n",
    "        e4 = self.relu(e4)         # 32 --> 32\n",
    "        x = self.pool4(e4)         # 32 --> 16\n",
    "        \n",
    "        x = self.enc5_1(x)\n",
    "        x = self.relu(x)\n",
    "        e5 = self.enc5_2(x)\n",
    "        e5 = self.relu(e5)         # 16 --> 16\n",
    "        x = self.pool5(e5)         # 16 --> 8\n",
    "        \n",
    "        \n",
    "        \n",
    "        flatten_x = torch.flatten(x, start_dim=1)  \n",
    "        flatten_x = self.fc0(flatten_x)\n",
    "        flatten_x = self.relu(flatten_x)\n",
    "        \n",
    "        \n",
    "        # Condition\n",
    "        y = torch.flatten(y, start_dim = 1)\n",
    "        y = torch.cat([flatten_x, y], dim=1)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc3(y)\n",
    "        y = self.relu(y)\n",
    "\n",
    "        \n",
    "        \n",
    "        cond = y.repeat(1, x.shape[2]*x.shape[2])\n",
    "        cond = torch.reshape(cond, (-1,512, x.shape[2], x.shape[2]))\n",
    "\n",
    "        \n",
    "        # Decoder Network\n",
    "        x = self.upsample5(cond)\n",
    "        x = torch.cat([x, e5], dim=1)         # (512,16,16) + (512,16,16) --> (1024,16,16)\n",
    "        x = self.dec5_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dec5_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.upsample4(x)\n",
    "        x = torch.cat([x, e4], dim=1)\n",
    "        x = self.dec4_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dec4_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat([x, e3], dim=1)\n",
    "        x = self.dec3_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dec3_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat([x, e2], dim=1)\n",
    "        x = self.dec2_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dec2_2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.upsample1(x)\n",
    "        x = torch.cat([x, e1], dim=1)\n",
    "        x = self.dec1_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dec1_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dec1_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01791b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "my_model = UNet().cpu()\n",
    "summary(my_model, [(3,256,256),(5,1,1)], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "  print(\"gpu up\")\n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1680a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = UNet().to(device)\n",
    "\n",
    "\n",
    "# Load the Dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "optim = Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dff997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "test_in, test_out, cond = next(iter(test_dataloader))\n",
    "\n",
    "case_control_num = 15\n",
    "\n",
    "test_in = test_in[case_control_num]\n",
    "test_in = torch.unsqueeze(test_in, dim=0)\n",
    "test_out = test_out[case_control_num]\n",
    "test_out = torch.unsqueeze(test_out, dim=0)\n",
    "cond = cond[case_control_num]\n",
    "cond = torch.unsqueeze(cond, dim=0)\n",
    "\n",
    "print({test_in.size()})\n",
    "print({test_out.size()})\n",
    "print({cond.size()})\n",
    "print(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(300):\n",
    "    iterator = tqdm.tqdm(train_dataloader)\n",
    "    \n",
    "    for data, label, cond in iterator:                 \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        pred = model(data.to(device), cond.to(device))  \n",
    "        \n",
    "        loss = nn.MSELoss()(pred, label.to(device))    \n",
    "        loss.backward()                                \n",
    "        optim.step()                                   \n",
    "        \n",
    "        iterator.set_description(f\"epoch:{epoch+1} loss:{loss.item()}\")\n",
    "        \n",
    "torch.save(model.state_dict(), \"./UNet.pth\")           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e43604",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    model.load_state_dict(torch .load(\"UNet_paper Net_nomalized(256_256)(512,8,8)_OK_231017.pth\", map_location = \"cpu\"))   \n",
    "    \n",
    "    \n",
    "    pred_image = model(test_in.to(device), cond.to(device))\n",
    "    pred_image = torch.squeeze(pred_image, dim=0)\n",
    "    pred_image = pred_image.transpose(2,0)\n",
    "    \n",
    "    test_out = torch.squeeze(test_out, dim=0)\n",
    "    test_out = test_out.transpose(2,0)  \n",
    "    \n",
    "print(pred_image.size())\n",
    "print(test_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_out.cpu())\n",
    "plt.title(f\"real image : {cond}\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pred_image.cpu())\n",
    "plt.title(f\"predicted image : {cond}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e2a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "cond = [4.49, 2.57, 2.95, 1.44, 1.9]                            \n",
    "cond = torch.tensor(cond)\n",
    "cond = torch.unsqueeze(cond, dim=0)\n",
    "img = model(test_in.to(device), cond.to(device))\n",
    "img = torch.squeeze(img, dim=0)\n",
    "img = img.cpu().transpose(2,0)\n",
    "img = img.detach().numpy()\n",
    "plt.title(cond[0:5])\n",
    "plt.imshow(img)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "cond = [4.48, 2.4, 2.4, 1.2, 1.2]                             \n",
    "cond = torch.tensor(cond)\n",
    "cond = torch.unsqueeze(cond, dim=0)\n",
    "img = model(test_in.to(device), cond.to(device))\n",
    "img = torch.squeeze(img, dim=0)\n",
    "img = img.cpu().transpose(2,0)\n",
    "img = img.detach().numpy()\n",
    "plt.title(cond[0:5])\n",
    "plt.imshow(img)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "cond = [6, 2.4, 2.4, 2.4, 2.4]                             \n",
    "cond = torch.tensor(cond)\n",
    "cond = torch.unsqueeze(cond, dim=0)\n",
    "img = model(test_in.to(device), cond.to(device))\n",
    "img = torch.squeeze(img, dim=0)\n",
    "img = img.cpu().transpose(2,0)\n",
    "img = img.detach().numpy()\n",
    "plt.title(cond[0:5])\n",
    "plt.imshow(img)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "cond = [6.72, 2.4, 2.4, 2.4, 2.4]                              \n",
    "cond = torch.tensor(cond)\n",
    "cond = torch.unsqueeze(cond, dim=0)\n",
    "img = model(test_in.to(device), cond.to(device))\n",
    "img = torch.squeeze(img, dim=0)\n",
    "img = img.cpu().transpose(2,0)\n",
    "img = img.detach().numpy()\n",
    "plt.title(cond[0:5])\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
